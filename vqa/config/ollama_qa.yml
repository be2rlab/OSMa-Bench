data_dir: "./data"
output_dir: "./output"

foundation_model:
    api_name: "ollama"
    llm: "llama3.1:8b"
    lvlm: "llava:7b"

description_generation:
    prompt: >
        Describe the given image and ALL the objects using structured statements.
        For EVERY visible object, append affordance tags in square brackets. Use the verbs: open, pull, push, sit, place, grasp, store, lie.

        If more than 50% of the scene is blocked by walls, return:
        '{rejection_keyword}'.

        Use only these two formats:
        - {detailed object description} {object} is {spatial relationship} {other object}.
        - {detailed object description} {object} [affordance tag] is present in the scene.

        Do not describe objects using vague location references like "in the background", "in the foreground", "on the left", "on the right", or "at the center".  
        Do not use standalone positional statements like "X is in the background". Instead, always use a relation to **another object** (e.g., "Window is behind the bed").

        Additionally, add the following annotations:
        - If the floor patch in front of an object is visibly clear, add line: {free-space in front of <object>: true}
        - If there is at least one hand-width (≈ 10 cm) of empty space above an object, add line: {clearance-above <object>: true}

        Ensure:
        - All visible objects are described with attributes (color, material, size, shape, function).
        - Spatial relationships are logically consistent and involve exactly two objects.
        - Use broad and recognizable object names. Avoid ambiguous references.
        - Return only the structured statements.
    rejection_keyword: "Scene not suitable"

qa_generation:
    prompt: >
        {
            "instructions": "Generate diverse questions based strictly on the given scene description. Do not infer or assume any properties beyond the provided information.",
            "categories": [
                {
                    "name": "Existence-Positive",
                    "count": "3-5",
                    "examples": ["Is there X placed on Y?", "Is there an X in Y?"],
                    "rules": "Questions about objects that DO exist in the scene. Answers must be 'Yes'."
                },
                {
                    "name": "Existence-Negative",
                    "count": "2-4",
                    "examples": ["Is there X placed on Y?", "Is there an X in Y?"],
                    "rules": "Ask about non-existent but plausible objects. Answers must be 'No'."
                },
                {
                    "name": "Binary Logical",
                    "count": "3-5",
                    "examples": [
                        "Is there an X AND a Y?",
                        "Is there an X OR a Y?"
                    ],
                    "rules": "Must include 'AND' or 'OR'. Answers must be 'Yes' or 'No'."
                },
                {
                    "name": "Object Attributes",
                    "count": "3-5",
                    "examples": [
                        "What color is X with a specific shape?",
                        "What material is X made of?"
                    ],
                    "rules": "Ensure clarity by specifying distinguishing properties."
                },
                {
                    "name": "Object Relations - Spatial",
                    "count": "3-5",
                    "examples": [
                        "What is positioned in front of X?",
                        "What is located under X?"
                    ],
                    "rules": "These questions must describe interactions (e.g., 'holds', 'supports'). Each question must reference exactly two specific objects (e.g. \"What is above the *red sofa*?\"). Do NOT ask about walls / floor / ceiling. Phrase it so that **only one object in the image** satisfies the relation."
                },
                {
                    "name": "Object Relations - Functional",
                    "count": "1-3",
                    "examples": [
                        "What does X interact with Y?",
                        "Which object serves as support for X?",
                        "Which object holds or contains X?"
                    ],
                    "rules": "These questions must describe interactions (e.g., 'holds', 'supports'). Each question must reference exactly two specific objects (e.g. \"What is above the *red sofa*?\"). Do NOT ask about walls / floor / ceiling. Phrase it so that **only one object in the image** satisfies the relation."
                },
                {
                    "name": "Measurement",
                    "count": "1-3",
                    "examples": ["How many X are present?", "How many X are grouped together?"],
                    "rules": "Answer **must be an Arabic numeral** (e.g. '0', '4'). If count is unknown or ambiguous – SKIP the question."
                },
                {
                    "name": "Task Reachability",
                    "count": "2-4",
                    "examples": ["Can the robot stand in front of the oven?"],
                    "rules": "Ask if the robot can STAND, DRIVE or PLACE ITSELF at a location observed in the image. Always answer 'Yes' or 'No'"
                },
                {
                    "name": "Object Affordance",
                    "count": "3-5",
                    "examples": ["Which objects can be opened?"],
                    "rules": "Use the affordance verbs extracted earlier in descriptions. Wh-questions are allowed"
                },
                {
                    "name": "Manipulation Pre-condition",
                    "count": "1-3",
                    "examples": ["Is there a graspable mug on a reachable surface?"],
                    "rules": "Combine affordance with reachability"
                },
                {
                    "name": "Comparison",
                    "count": "3-5",
                    "examples": ["Which is larger: X or Y?","Which is positioned higher: X or Y?"],
                    "rules": "Only use when both objects are visible and clearly distinguishable."
                }
            ],
            "format": "Return a JSON list with 'question', 'answer', and 'category'."
        }

qa_validation:
    neural_val_prompt: >
        Analyze the image and validate the given questions and answers strictly against it, ensuring no assumptions
        are made beyond the visual evidence. Additionally, cross-check the QA against the entire scene description
        to prevent inconsistencies. Remove questions that:
        - Contradict the image or scene description.
        - Require a specific viewpoint (e.g., 'Which is further back?', 'Which is positioned closer?', etc).
        - Have ambiguous or multiple correct answers.
        If multiple valid interpretations exist, select the one most consistent with both image and description.
        Return only the filtered and validated questions, keeping their categories and answers in JSON format.
    filter_non_objects_prompt: >
        Given the following words extracted from questions, filter out any words that are NOT objects.
        Exclude prepositions, adjectives, numbers, and vague terms. Return only object names in JSON list format.

cg_answering:
    prompt: >
        Answer the following questions based ONLY on the provided scene graph.
        DESCRIBE RESULT USING ONLY JSON FILE.
        Add an 'answer' field for each question with your response.
        IF Yes/No EXPECTED, ANSWER STRICLY 'Yes' OR 'No'.
        IF COUNTING, ANSWER STRICLY AS A NUMBER.

        Scene graph: ```json
        {scene_graph}
        ```

        Questions: ```json
        {questions}
        ```

        YOUR ANSWER EXAMPLE: ```json 
        [
            {
                "id": 100,
                "question": "Is there a lamp on the desk?", 
                "answer": "No"
            }, 
            {
                "id": 1703,
                "question": "What material is the desk made of?", 
                "answer": "Metal"
            }
        ]
        ```

evaluation:
    prompt: >
        Compare ground truth answers ('answer') and scene-graph answers ('scene_graph_answer').
        Add a 'similar':'Yes'/'No' field to each entry.
        Questions: ```json
        {questions}
        ```